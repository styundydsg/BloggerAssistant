from __future__ import annotations
import pandas as pd
from openai import OpenAI
import os
import sys

import panel as pn
import langchain
from langchain_openai import ChatOpenAI
from langchain.chains.retrieval_qa.base import RetrievalQA #æ£€ç´¢QAé“¾ï¼Œåœ¨æ–‡æ¡£ä¸Šè¿›è¡Œæ£€ç´¢
from langchain_community.document_loaders import CSVLoader
from langchain_community.vectorstores import DocArrayInMemorySearch
from sentence_transformers import SentenceTransformer
from langchain_huggingface import HuggingFaceEmbeddings
from langchain.indexes import VectorstoreIndexCreator
from langchain.evaluation.qa import QAGenerateChain
from langchain.base_language import BaseLanguageModel
from langchain.output_parsers.regex import RegexParser
from langchain.prompts import PromptTemplate
from langchain.evaluation.qa import QAEvalChain
from langchain_community.document_loaders import UnstructuredMarkdownLoader,DirectoryLoader
from langchain_community.vectorstores import FAISS

import pickle
import frontmatter

from typing import Any
from IPython.display import display, Markdown #åœ¨jupyteræ˜¾ç¤ºä¿¡æ¯çš„å·¥å…·

langchain.debug = False
parent_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
model_dir = os.path.join(parent_dir, 'model')
sys.path.append(model_dir)
sys.path.append('../..')

pn.extension()

CONFIG = {
    "API_KEY": os.getenv("DEEPSEEK_API_KEY"),  # ä»ç¯å¢ƒå˜é‡è·å–
    "BASE_URL": "https://api.deepseek.com",
    "HISTORY_DIR": os.path.expanduser("~/deepseek_chat_history"),  # è·¨å¹³å°è·¯å¾„
    "MODEL_MAPPING": {
        "V3": "deepseek-chat",
        "R1": "deepseek-reasoner"
    }
}
client = OpenAI(api_key=CONFIG["API_KEY"], base_url=CONFIG["BASE_URL"])

def get_completion_from_messages(messages, 
                                 model=CONFIG["MODEL_MAPPING"]["V3"], 
                                 temperature=0, 
                                 max_tokens=1000):
    '''

    å‚æ•°: 
    messages: è¿™æ˜¯ä¸€ä¸ªæ¶ˆæ¯åˆ—è¡¨ï¼Œæ¯ä¸ªæ¶ˆæ¯éƒ½æ˜¯ä¸€ä¸ªå­—å…¸ï¼ŒåŒ…å« role(è§’è‰²ï¼‰å’Œ content(å†…å®¹)ã€‚è§’è‰²å¯ä»¥æ˜¯'system'ã€'user' æˆ– 'assistantâ€™ï¼Œå†…å®¹æ˜¯è§’è‰²çš„æ¶ˆæ¯ã€‚
    model: è°ƒç”¨çš„æ¨¡å‹ï¼Œé»˜è®¤ä¸º gpt-3.5-turbo(ChatGPT)ï¼Œæœ‰å†…æµ‹èµ„æ ¼çš„ç”¨æˆ·å¯ä»¥é€‰æ‹© gpt-4
    temperature: è¿™å†³å®šæ¨¡å‹è¾“å‡ºçš„éšæœºç¨‹åº¦ï¼Œé»˜è®¤ä¸º0ï¼Œè¡¨ç¤ºè¾“å‡ºå°†éå¸¸ç¡®å®šã€‚å¢åŠ æ¸©åº¦ä¼šä½¿è¾“å‡ºæ›´éšæœºã€‚
    max_tokens: è¿™å†³å®šæ¨¡å‹è¾“å‡ºçš„æœ€å¤§çš„ token æ•°ã€‚
    '''
    response = client.chat.completions.create(
    model=model,  # ä½¿ç”¨ä¼ å…¥çš„modelå‚æ•°
    messages=messages,
    temperature=temperature,
    max_tokens=max_tokens,
    stream=False
    )
    return response.choices[0].message.content

llm = ChatOpenAI(temperature=0,
                  model="deepseek-chat",  # æ¨¡å‹åç§°
    openai_api_key=os.getenv("DEEPSEEK_API_KEY", "sk-your-key-here"),  # DeepSeek çš„ API Key
    openai_api_base="https://api.deepseek.com")  # DeepSeek çš„æ¥å£åœ°å€)

delimiter = "####"

try:
    # ä½¿ç”¨ç»å¯¹è·¯å¾„åŠ è½½blog_files
    blog_files_path = os.path.join(parent_dir, 'blog_files')
    loader = DirectoryLoader(
        path=blog_files_path,
        glob='**/*.md',
        loader_cls=UnstructuredMarkdownLoader,
        loader_kwargs={'encoding': 'utf-8', 'mode': 'elements'},  # å¢åŠ æ¨¡å¼å‚æ•°
        show_progress=True,
        silent_errors=True,  # å¿½ç•¥é”™è¯¯æ–‡ä»¶
        use_multithreading=True
    )
    data = loader.load()
    
    # åˆ›å»ºå­—å…¸å­˜å‚¨æ¯ä¸ªæ–‡ä»¶çš„categoriesä¿¡æ¯
    file_categories = {}
    
    # è·å–æ‰€æœ‰å”¯ä¸€çš„mdæ–‡ä»¶è·¯å¾„
    md_files = set()
    for doc in data:
        source_path = doc.metadata['source']
        if source_path.endswith('.md'):
            md_files.add(source_path)
    
    # è§£ææ¯ä¸ªmdæ–‡ä»¶çš„å‰è¨€éƒ¨åˆ†è·å–categories
    for md_file in md_files:
        try:
            with open(md_file, 'r', encoding='utf-8') as f:
                content = f.read()
            post = frontmatter.loads(content)
            categories = post.metadata.get('categories', 'æœªåˆ†ç±»')
            file_categories[md_file] = categories
        except Exception as e:
            print(f"è§£ææ–‡ä»¶ {md_file} çš„å‰è¨€å¤±è´¥: {str(e)}")
            file_categories[md_file] = 'æœªåˆ†ç±»'
    
    # ä¸ºæ¯ä¸ªæ–‡æ¡£ç‰‡æ®µæ·»åŠ categoriesä¿¡æ¯
    for doc in data:
        source_path = doc.metadata['source']
        doc.metadata['last_modified'] = os.path.getmtime(source_path)  # è®°å½•æ–‡ä»¶ä¿®æ”¹æ—¶é—´
        # æ·»åŠ categoriesä¿¡æ¯åˆ°å…ƒæ•°æ®
        doc.metadata['file_categories'] = file_categories.get(source_path, 'æœªåˆ†ç±»')
    
    print(f"æˆåŠŸåŠ è½½ {len(data)} ä¸ªæ–‡æ¡£ç‰‡æ®µ")
    print(f"è§£æäº† {len(file_categories)} ä¸ªæ–‡ä»¶çš„categoriesä¿¡æ¯")
except Exception as e:
    print(f"æ–‡æ¡£åŠ è½½å¤±è´¥: {str(e)}")
    # ä¸é€€å‡ºï¼Œè€Œæ˜¯åˆ›å»ºä¸€ä¸ªç©ºçš„æ•°æ®åˆ—è¡¨ç»§ç»­è¿è¡Œ
    data = []

template = """åŸºäºä»¥ä¸‹æ–‡æ¡£å†…å®¹ï¼Œç”Ÿæˆä¸€ä¸ªé—®é¢˜åŠå…¶ç­”æ¡ˆï¼š\
{doc}\
\
è¯·ä¸¥æ ¼ä½¿ç”¨ä»¥ä¸‹æ ¼å¼ï¼š\
QUESTION: [ä½ çš„é—®é¢˜]\
ANSWER: [å¯¹åº”çš„ç­”æ¡ˆ]"""


output_parser = RegexParser(
    regex=r"QUESTION:\s\*(.\*?)\s\*?ANSWER:\s\*(.\*)",  # æ”¹è¿›æ­£åˆ™è¡¨è¾¾å¼
    output_keys=["query", "answer"]
)

PROMPT = PromptTemplate(
    input_variables=["doc"], 
    template=template, 
    output_parser=output_parser
)


class MyQAGenerateChain(QAGenerateChain):
    """è‡ªå®šä¹‰QAç”Ÿæˆé“¾"""
    @classmethod
    def from_llm(cls, llm: BaseLanguageModel, **kwargs: Any) -> QAGenerateChain:
        return cls(llm=llm, prompt=PROMPT, **kwargs)


example_gen_chain = MyQAGenerateChain.from_llm(llm)


embeddings = HuggingFaceEmbeddings(model_name=model_dir) #åˆå§‹åŒ–

vectorstore_path = "faiss_index"
if os.path.exists(vectorstore_path) and os.path.isdir(vectorstore_path):
    try:
        vectorstore = FAISS.load_local(
            vectorstore_path, 
            embeddings, 
            allow_dangerous_deserialization=True
        )
        print("æˆåŠŸåŠ è½½ç°æœ‰å‘é‡æ•°æ®åº“")

        existing_files = {doc.metadata['source'] for doc in vectorstore.docstore._dict.values()}
        current_files = {doc.metadata['source'] for doc in data}
        new_files = current_files - existing_files
        modified_files = set()

        for doc in data:
            source = doc.metadata['source']
            if source in existing_files:
                existing_doc = next(d for d in vectorstore.docstore._dict.values() 
                                   if d.metadata['source'] == source)
                if doc.metadata['last_modified'] > existing_doc.metadata.get('last_modified', 0):
                    modified_files.add(source)
        
        if new_files or modified_files:
            updated_docs = [doc for doc in data 
                           if doc.metadata['source'] in (new_files | modified_files)]
            vectorstore.add_documents(updated_docs)
            print(f"ğŸ†• å¢é‡æ›´æ–° {len(updated_docs)} ä¸ªæ–‡æ¡£ç‰‡æ®µ")
            vectorstore.save_local(vectorstore_path)
        else:
            print("â© æœªæ£€æµ‹åˆ°æ–‡ä»¶å˜æ›´ï¼Œæ— éœ€æ›´æ–°")

    except:
        print("å‘é‡æ•°æ®åº“æŸåï¼Œé‡å»ºä¸­...")
        vectorstore = None
else:
    vectorstore = None

if not vectorstore:
    # ä½¿ç”¨æ‰€æœ‰æ–‡æ¡£æ„å»ºç´¢å¼•
    index_creator = VectorstoreIndexCreator(
        embedding=embeddings,
        vectorstore_cls=FAISS,
    )
    index = index_creator.from_documents(data)
    vectorstore = index.vectorstore
    vectorstore.save_local(vectorstore_path)
    print(f"å·²åˆ›å»ºæ–°çš„å‘é‡æ•°æ®åº“ï¼ŒåŒ…å« {len(vectorstore.index_to_docstore_id)} ä¸ªæ–‡æ¡£")
    

qa = RetrievalQA.from_chain_type(
    llm=llm, 
    chain_type="stuff", 
    retriever=vectorstore.as_retriever(search_kwargs={"k": 3}),  # è¿”å›å‰3ä¸ªç›¸å…³æ–‡æ¡£
    verbose=True,
    return_source_documents=True,  # è¿”å›æºæ–‡æ¡£
    chain_type_kwargs={
        "prompt": PromptTemplate(
            input_variables=["context", "question"],
            template="""\
            ä½¿ç”¨ä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ã€‚å¦‚æœä½ ä¸çŸ¥é“ç­”æ¡ˆï¼Œå°±è¯´ä¸çŸ¥é“ã€‚\
            \
            ä¸Šä¸‹æ–‡:\
            {context}\
            \
            é—®é¢˜: {question}\
            ç­”æ¡ˆ:\
            """
        )
    }
)

def ask_question(question: str):
    """å‘æ–‡æ¡£æé—®å¹¶è·å–å›ç­”"""
    try:
        result = qa.invoke({"query": question})
        answer = result["result"]
        
        # æ˜¾ç¤ºæ¥æºæ–‡æ¡£ - ä½¿ç”¨æ›´åˆé€‚çš„å…ƒæ•°æ®å­—æ®µ
        sources_list = []
        for doc in result["source_documents"]:
            # ä½¿ç”¨æ–‡ä»¶åè€Œä¸æ˜¯å®Œæ•´è·¯å¾„
            filename = doc.metadata.get('filename', os.path.basename(doc.metadata.get('source', 'æœªçŸ¥æ–‡ä»¶')))
            
            # ä½¿ç”¨æ–‡ä»¶ä¸­çš„categorieså­—æ®µä¿¡æ¯
            category = doc.metadata.get('file_categories', 'æœªåˆ†ç±»')
            
            sources_list.append(f"- {filename} ({category})")
        
        sources = "\n\næ¥æºæ–‡æ¡£:\n" + "\n".join(sources_list)
        
        return answer + sources
    except Exception as e:
        return f"æŸ¥è¯¢å¤±è´¥: {str(e)}"

# 8. ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # æµ‹è¯•æé—®
    print("\n=== æ–‡æ¡£é—®ç­”ç³»ç»Ÿ ===")
    while True:
        user_question = input("\nä½ çš„é—®é¢˜ (è¾“å…¥qé€€å‡º): ")
        if user_question.lower() == 'q':
            break
            
        response = ask_question(user_question)
        print(f"\nå›ç­”: {response}")


# test_data = pd.read_csv(file,header=None)

# print(data[11])



# embedding_model = HuggingFaceEmbeddings(
#     model_name="BAAI/bge-large-zh-v1.5",
#     model_kwargs={"device": "cpu"},
#     encode_kwargs={"normalize_embeddings": True}
# )





# db = z.from_documents(docs, embedding=embedding_model)

# query = "Please suggest a shirt with sunblocking"

# docs = db.similarity_search(query)

# retriever = db.as_retriever()

# qdocs = "".join([docs[i].page_content for i in range(len(docs))])



# query =  "Please list all your shirts with sun protection in a table \
# in markdown and summarize each one."

# response = qa_stuff.invoke(query)

# # response = llm.call_as_llm(f"{qdocs} Question: Please list all your \
# # shirts with sun protection in a table in markdown and summarize each one.") 

# print(response)
