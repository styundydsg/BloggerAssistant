"""æ„å›¾è¯†åˆ«æ¨¡å— - ä¿®å¤ç‰ˆæœ¬"""

import json
import torch
import torch.nn as nn
import numpy as np
from typing import Dict, Any, List, Optional
import os
import pickle
import logging
import re
from pathlib import Path
from collections import Counter
import jieba
from .config import CONFIG

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# è§£å†³numpyå…¼å®¹æ€§é—®é¢˜
try:
    np.importlib.reload(np)
except:
    pass


class SimpleIntentClassifier(nn.Module):
    """ç®€åŒ–æ„å›¾åˆ†ç±»å™¨æ¨¡å‹"""
    
    def __init__(self, vocab_size: int, embedding_dim: int, hidden_dim: int, 
                 output_dim: int, n_layers: int, dropout: float):
        super().__init__()
        
        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        
        # å¦‚æœåªæœ‰ä¸€å±‚ï¼Œä¸ä½¿ç”¨dropout
        if n_layers == 1:
            dropout = 0.0
            
        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, 
                           dropout=dropout, batch_first=True, bidirectional=True)
        self.fc = nn.Linear(hidden_dim * 2, output_dim)
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, text: torch.Tensor) -> torch.Tensor:
        embedded = self.dropout(self.embedding(text))
        output, (hidden, cell) = self.lstm(embedded)
        hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)
        return self.fc(hidden)


class SimpleVocabulary:
    """ç®€åŒ–è¯æ±‡è¡¨å¤„ç†"""
    
    def __init__(self):
        self.word2idx = {}
        self.idx2word = {}
        self.word_counts = Counter()
        self.pad_token = '<PAD>'
        self.unk_token = '<UNK>'
        self._build_basic_vocab()
    
    def _build_basic_vocab(self):
        """æ„å»ºåŸºç¡€è¯æ±‡è¡¨"""
        basic_tokens = [self.pad_token, self.unk_token]
        for idx, token in enumerate(basic_tokens):
            self.word2idx[token] = idx
            self.idx2word[idx] = token
    
    def build_vocab(self, texts: List[str], min_freq: int = 1):  # é™ä½æœ€å°è¯é¢‘
        """ä»æ–‡æœ¬æ„å»ºè¯æ±‡è¡¨"""
        # é‡ç½®è¯æ±‡è¡¨
        self.word2idx = {self.pad_token: 0, self.unk_token: 1}
        self.idx2word = {0: self.pad_token, 1: self.unk_token}
        self.word_counts = Counter()
        
        # ç»Ÿè®¡è¯é¢‘
        for text in texts:
            tokens = self.tokenize(text)
            self.word_counts.update(tokens)
        
        # æ·»åŠ æ»¡è¶³æœ€å°è¯é¢‘çš„è¯
        idx = 2
        for word, count in self.word_counts.items():
            if count >= min_freq:
                self.word2idx[word] = idx
                self.idx2word[idx] = word
                idx += 1
        
        logger.info(f"è¯æ±‡è¡¨æ„å»ºå®Œæˆï¼Œè¯æ±‡é‡: {len(self.word2idx)}")
    
    def tokenize(self, text: str) -> List[str]:
        """ä¸­æ–‡åˆ†è¯"""
        try:
            # ä½¿ç”¨jiebaåˆ†è¯ï¼Œå¯ç”¨HMMä»¥æé«˜åˆ†è¯å‡†ç¡®æ€§
            return list(jieba.cut(text, HMM=True))
        except Exception as e:
            logger.warning(f"jiebaåˆ†è¯å¤±è´¥: {e}ï¼Œä½¿ç”¨ç®€å•åˆ†è¯")
            # å›é€€åˆ°ç®€å•åˆ†è¯ï¼šæŒ‰å­—ç¬¦åˆ†å‰²
            return [char for char in text if char.strip()]
    
    def numericalize(self, text: str, max_length: int = 50) -> List[int]:
        """å°†æ–‡æœ¬è½¬æ¢ä¸ºæ•°å­—åºåˆ—"""
        tokens = self.tokenize(text)
        numericalized = []
        
        for token in tokens:
            if token in self.word2idx:
                numericalized.append(self.word2idx[token])
            else:
                numericalized.append(self.word2idx[self.unk_token])
        
        # å¡«å……æˆ–æˆªæ–­
        if len(numericalized) < max_length:
            numericalized.extend([self.word2idx[self.pad_token]] * (max_length - len(numericalized)))
        else:
            numericalized = numericalized[:max_length]
        
        return numericalized
    
    def __len__(self):
        return len(self.word2idx)


class SimpleIntentRecognizer:
    """åŸºäºPyTorchçš„ç®€åŒ–æ„å›¾è¯†åˆ«å™¨"""
    
    def __init__(self):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        logger.info(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        self.model: Optional[SimpleIntentClassifier] = None
        self.vocab: Optional[SimpleVocabulary] = None
        self.intent_labels: Optional[List[str]] = None
        
        self.model_dir = Path(CONFIG["MODEL_DIR"])
        self.model_path = self.model_dir / "simple_intent_classifier.pth"
        self.vocab_path = self.model_dir / "simple_vocab.pkl"
        self.training_data_path = Path(__file__).parent.parent.parent / "intent_training_data.json"
        
        # æ¨¡å‹å‚æ•° - ä¼˜åŒ–é…ç½®
        self.embedding_dim = 50   # å‡å°åµŒå…¥ç»´åº¦
        self.hidden_dim = 64      # å‡å°éšè—å±‚å¤§å°
        self.output_dim = 5       # 5ç§æ„å›¾ç±»å‹
        self.n_layers = 1         # å•å±‚LSTM
        self.dropout = 0.2        # å‡å°dropout
        self.batch_size = 8       # å‡å°æ‰¹å¤§å°
        self.epochs = 20          # å‡å°‘è®­ç»ƒè½®æ•°
        self.max_length = 30      # å‡å°åºåˆ—é•¿åº¦
        self.learning_rate = 0.001
        
        # æ„å›¾å…³é”®è¯æ˜ å°„
        self._setup_intent_keywords()
        self._load_or_train_model()
    
    def _setup_intent_keywords(self):
        """è®¾ç½®æ„å›¾å…³é”®è¯æ˜ å°„"""
        self.contact_keywords = [
            'è”ç³»', 'åšä¸»', 'äººå·¥', 'å®¢æœ', 'å¸®åŠ©', 'æ”¯æŒ', 'email', 'é‚®ç®±', 
            'å¾®ä¿¡', 'qq', 'ç”µè¯', 'è”ç³»æ–¹å¼', 'contact', 'help', 'support',
            'è”ç³»ä½ ', 'æ‰¾ä½ ', 'æ‰¾ä½ ', 'æ‰¾ä½ '
        ]
        
        self.tech_keywords = {
            "ç¼–ç¨‹è¯­è¨€": ["python", "java", "c++", "cè¯­è¨€", "javascript", "typescript", "ç¼–ç¨‹", "ä»£ç "],
            "æ“ä½œç³»ç»Ÿ": ["linux", "windows", "macos", "unix", "xv6", "openharmony", "ç³»ç»Ÿ", "æ“ä½œç³»ç»Ÿ"],
            "å¼€å‘å·¥å…·": ["git", "docker", "vscode", "ide", "ç¼–è¯‘å™¨", "å·¥å…·"],
            "ç¡¬ä»¶": ["èŠ¯ç‰‡", "å¤„ç†å™¨", "å†…å­˜", "ç¡¬ç›˜", "ä¸»æ¿", "ç¡¬ä»¶"],
            "ç½‘ç»œ": ["tcp", "udp", "http", "https", "åè®®", "ç½‘ç»œ"]
        }
        
        self.question_types = {
            "æ¦‚å¿µè§£é‡Š": ["ä»€ä¹ˆ", "æ˜¯ä»€ä¹ˆ", "å®šä¹‰", "æ¦‚å¿µ", "æ„æ€", "å«ä¹‰"],
            "ä½¿ç”¨æ–¹æ³•": ["æ€ä¹ˆ", "å¦‚ä½•", "ä½¿ç”¨", "ç”¨æ³•", "æ€æ ·", "æ“ä½œ"],
            "é—®é¢˜è§£å†³": ["è§£å†³", "é”™è¯¯", "é—®é¢˜", "å¤±è´¥", "æ€ä¹ˆåŠ", "ä¸ºå•¥", "ä¸ºä»€ä¹ˆ"],
            "ä»£ç ç¤ºä¾‹": ["ä»£ç ", "ç¤ºä¾‹", "å®ä¾‹", "demo", "ä¾‹å­", "æºç "]
        }
        
        # æ„å›¾ä¼˜å…ˆçº§æ˜ å°„
        self.intent_priority = {
            "è”ç³»åšä¸»": 3,  # æœ€é«˜ä¼˜å…ˆçº§
            "åšå®¢å†…å®¹æŸ¥è¯¢": 2,
            "æŠ€æœ¯é—®ç­”": 1   # é»˜è®¤ä¼˜å…ˆçº§
        }
    
    def _load_or_train_model(self):
        """åŠ è½½æˆ–è®­ç»ƒæ¨¡å‹"""
        try:
            # ç¡®ä¿æ¨¡å‹ç›®å½•å­˜åœ¨
            self.model_dir.mkdir(parents=True, exist_ok=True)
            
            if self._model_files_exist():
                self._load_model()
                logger.info("æ„å›¾è¯†åˆ«æ¨¡å‹åŠ è½½æˆåŠŸ")
            else:
                logger.info("æœªæ‰¾åˆ°é¢„è®­ç»ƒæ¨¡å‹ï¼Œå¼€å§‹è®­ç»ƒ...")
                self._train_model()
                
        except Exception as e:
            logger.error(f"æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
            # å³ä½¿è®­ç»ƒå¤±è´¥ï¼Œä¹Ÿå…è®¸ä½¿ç”¨å…³é”®è¯å›é€€
            logger.info("å°†ä½¿ç”¨å…³é”®è¯å›é€€æ¨¡å¼")
    
    def _model_files_exist(self) -> bool:
        """æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨"""
        return self.model_path.exists() and self.vocab_path.exists()
    
    def _load_training_data(self) -> List[Dict]:
        """åŠ è½½è®­ç»ƒæ•°æ®"""
        if not self.training_data_path.exists():
            logger.warning(f"è®­ç»ƒæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {self.training_data_path}")
            # åˆ›å»ºé»˜è®¤è®­ç»ƒæ•°æ®
            return self._create_default_training_data()
        
        try:
            with open(self.training_data_path, 'r', encoding='utf-8') as f:
                training_data = json.load(f)
            
            train_data = []
            for item in training_data.get("training_data", []):
                train_data.append({
                    "text": item["input"],
                    "label": item["intent"]
                })
            
            logger.info(f"åŠ è½½äº† {len(train_data)} æ¡è®­ç»ƒæ•°æ®")
            return train_data
            
        except Exception as e:
            logger.error(f"åŠ è½½è®­ç»ƒæ•°æ®å¤±è´¥: {e}")
            return self._create_default_training_data()
    
    def _create_default_training_data(self) -> List[Dict]:
        """åˆ›å»ºé»˜è®¤è®­ç»ƒæ•°æ®"""
        default_data = [
            {"text": "æ€ä¹ˆè”ç³»åšä¸»", "label": "è”ç³»åšä¸»"},
            {"text": "æœ‰å¾®ä¿¡å—", "label": "è”ç³»åšä¸»"},
            {"text": "é‚®ç®±æ˜¯å¤šå°‘", "label": "è”ç³»åšä¸»"},
            {"text": "æ€ä¹ˆæ‰¾ä½ ", "label": "è”ç³»åšä¸»"},
            {"text": "Pythonç¼–ç¨‹é—®é¢˜", "label": "æŠ€æœ¯é—®ç­”"},
            {"text": "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ", "label": "æŠ€æœ¯é—®ç­”"},
            {"text": "æ€ä¹ˆä½¿ç”¨git", "label": "æŠ€æœ¯é—®ç­”"},
            {"text": "ä»£ç æŠ¥é”™æ€ä¹ˆåŠ", "label": "æŠ€æœ¯é—®ç­”"},
            {"text": "æŸ¥çœ‹åšå®¢æ–‡ç« ", "label": "åšå®¢å†…å®¹æŸ¥è¯¢"},
            {"text": "æœ‰ä»€ä¹ˆæŠ€æœ¯æ•™ç¨‹", "label": "åšå®¢å†…å®¹æŸ¥è¯¢"},
            {"text": "å­¦ä¹ ç¬”è®°", "label": "åšå®¢å†…å®¹æŸ¥è¯¢"}
        ]
        logger.info("ä½¿ç”¨é»˜è®¤è®­ç»ƒæ•°æ®")
        return default_data
    
    def _prepare_training_data(self):
        """å‡†å¤‡è®­ç»ƒæ•°æ®"""
        train_data = self._load_training_data()
        
        if not train_data:
            raise ValueError("æ²¡æœ‰å¯ç”¨çš„è®­ç»ƒæ•°æ®")
        
        # æå–æ–‡æœ¬å’Œæ ‡ç­¾
        texts = [item["text"] for item in train_data]
        labels = [item["label"] for item in train_data]
        
        # æ„å»ºè¯æ±‡è¡¨
        self.vocab = SimpleVocabulary()
        self.vocab.build_vocab(texts, min_freq=1)  # é™ä½æœ€å°è¯é¢‘
        
        # æ„å»ºæ ‡ç­¾æ˜ å°„
        unique_labels = list(set(labels))
        self.intent_labels = unique_labels
        self.output_dim = len(unique_labels)
        self.label2idx = {label: idx for idx, label in enumerate(unique_labels)}
        self.idx2label = {idx: label for label, idx in self.label2idx.items()}
        
        logger.info(f"è®­ç»ƒæ•°æ®: {len(texts)} æ¡æ–‡æœ¬, {len(unique_labels)} ç§æ„å›¾: {unique_labels}")
        
        return texts, labels
    
    def _create_data_loader(self, texts: List[str], labels: List[str]):
        """åˆ›å»ºæ•°æ®åŠ è½½å™¨"""
        # è½¬æ¢ä¸ºæ•°å€¼åºåˆ—
        text_sequences = []
        label_indices = []
        
        for text, label in zip(texts, labels):
            numericalized = self.vocab.numericalize(text, self.max_length)
            text_sequences.append(numericalized)
            label_indices.append(self.label2idx[label])
        
        # è½¬æ¢ä¸ºtensor
        text_tensor = torch.LongTensor(text_sequences)
        label_tensor = torch.LongTensor(label_indices)
        
        # åˆ›å»ºç®€å•æ•°æ®é›†
        dataset = torch.utils.data.TensorDataset(text_tensor, label_tensor)
        data_loader = torch.utils.data.DataLoader(
            dataset, 
            batch_size=self.batch_size,
            shuffle=True
        )
        
        return data_loader
    
    def _train_model(self):
        """è®­ç»ƒæ¨¡å‹"""
        logger.info("å¼€å§‹è®­ç»ƒæ„å›¾è¯†åˆ«æ¨¡å‹...")
        
        try:
            # å‡†å¤‡æ•°æ®
            texts, labels = self._prepare_training_data()
            data_loader = self._create_data_loader(texts, labels)
            
            # åˆå§‹åŒ–æ¨¡å‹
            vocab_size = len(self.vocab)
            self.model = SimpleIntentClassifier(
                vocab_size, 
                self.embedding_dim, 
                self.hidden_dim, 
                self.output_dim, 
                self.n_layers, 
                self.dropout
            )
            self.model = self.model.to(self.device)
            
            # è®­ç»ƒè¿‡ç¨‹
            optimizer = torch.optim.Adam(self.model.parameters(), lr=self.learning_rate)
            criterion = nn.CrossEntropyLoss()
            
            self.model.train()
            for epoch in range(self.epochs):
                total_loss = 0
                correct_predictions = 0
                total_predictions = 0
                
                for batch_texts, batch_labels in data_loader:
                    batch_texts = batch_texts.to(self.device)
                    batch_labels = batch_labels.to(self.device)
                    
                    optimizer.zero_grad()
                    predictions = self.model(batch_texts)
                    loss = criterion(predictions, batch_labels)
                    loss.backward()
                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                    optimizer.step()
                    
                    total_loss += loss.item()
                    
                    # è®¡ç®—å‡†ç¡®ç‡
                    _, predicted = torch.max(predictions, 1)
                    correct_predictions += (predicted == batch_labels).sum().item()
                    total_predictions += batch_labels.size(0)
                
                accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0
                avg_loss = total_loss / len(data_loader)
                
                if (epoch + 1) % 5 == 0:
                    logger.info(f'Epoch: {epoch+1}/{self.epochs}, Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}')
            
            # ä¿å­˜æ¨¡å‹
            self._save_model()
            logger.info("æ¨¡å‹è®­ç»ƒå®Œæˆ")
            
        except Exception as e:
            logger.error(f"æ¨¡å‹è®­ç»ƒå¤±è´¥: {e}")
            # è®­ç»ƒå¤±è´¥æ—¶ï¼Œè®¾ç½®æ¨¡å‹ä¸ºNoneï¼Œä½¿ç”¨å›é€€æ–¹æ³•
            self.model = None
    
    def _save_model(self):
        """ä¿å­˜æ¨¡å‹å’Œè¯æ±‡è¡¨"""
        try:
            # ä¿å­˜æ¨¡å‹
            torch.save({
                'model_state_dict': self.model.state_dict(),
                'model_config': {
                    'embedding_dim': self.embedding_dim,
                    'hidden_dim': self.hidden_dim,
                    'output_dim': self.output_dim,
                    'n_layers': self.n_layers,
                    'dropout': self.dropout
                }
            }, self.model_path)
            
            # ä¿å­˜è¯æ±‡è¡¨å’Œæ ‡ç­¾
            vocab_data = {
                'vocab': self.vocab,
                'intent_labels': self.intent_labels,
                'label2idx': self.label2idx,
                'idx2label': self.idx2label
            }
            with open(self.vocab_path, 'wb') as f:
                pickle.dump(vocab_data, f)
                
            logger.info(f"æ¨¡å‹å’Œè¯æ±‡è¡¨å·²ä¿å­˜")
            
        except Exception as e:
            logger.error(f"ä¿å­˜æ¨¡å‹å¤±è´¥: {e}")
            raise
    
    def _load_model(self):
        """åŠ è½½æ¨¡å‹å’Œè¯æ±‡è¡¨"""
        try:
            # åŠ è½½è¯æ±‡è¡¨å’Œæ ‡ç­¾
            with open(self.vocab_path, 'rb') as f:
                vocab_data = pickle.load(f)
            
            self.vocab = vocab_data['vocab']
            self.intent_labels = vocab_data['intent_labels']
            self.label2idx = vocab_data['label2idx']
            self.idx2label = vocab_data['idx2label']
            self.output_dim = len(self.intent_labels)
            
            # åŠ è½½æ¨¡å‹é…ç½®
            checkpoint = torch.load(self.model_path, map_location=self.device)
            model_config = checkpoint['model_config']
            
            # åˆå§‹åŒ–æ¨¡å‹
            vocab_size = len(self.vocab)
            self.model = SimpleIntentClassifier(
                vocab_size,
                model_config['embedding_dim'],
                model_config['hidden_dim'],
                model_config['output_dim'],
                model_config['n_layers'],
                model_config['dropout']
            )
            
            # åŠ è½½æ¨¡å‹æƒé‡
            self.model.load_state_dict(checkpoint['model_state_dict'])
            self.model = self.model.to(self.device)
            self.model.eval()
            
        except Exception as e:
            logger.error(f"åŠ è½½æ¨¡å‹å¤±è´¥: {e}")
            self.model = None
    
    def preprocess_text(self, text: str) -> torch.Tensor:
        """é¢„å¤„ç†æ–‡æœ¬"""
        if self.vocab is None:
            raise ValueError("è¯æ±‡è¡¨æœªåˆå§‹åŒ–")
        
        numericalized = self.vocab.numericalize(text, self.max_length)
        tensor = torch.LongTensor(numericalized).unsqueeze(0).to(self.device)
        return tensor
    
    def recognize_intent(self, user_input: str) -> Dict[str, Any]:
        """è¯†åˆ«ç”¨æˆ·è¾“å…¥çš„æ„å›¾"""
        # å¦‚æœæ¨¡å‹ä¸å¯ç”¨ï¼Œç›´æ¥ä½¿ç”¨å›é€€æ–¹æ³•
        if self.model is None or self.vocab is None:
            return self._fallback_intent_recognition(user_input)
        
        try:
            # é¢„å¤„ç†æ–‡æœ¬
            text_tensor = self.preprocess_text(user_input)
            
            # é¢„æµ‹
            with torch.no_grad():
                prediction = self.model(text_tensor)
                probabilities = torch.softmax(prediction, dim=1)
                confidence, predicted_idx = torch.max(probabilities, 1)
            
            intent_idx = predicted_idx.item()
            confidence_score = confidence.item()
            
            # è·å–æ„å›¾æ ‡ç­¾
            if 0 <= intent_idx < len(self.intent_labels):
                intent = self.idx2label[intent_idx]
            else:
                intent = "æŠ€æœ¯é—®ç­”"
                confidence_score = 0.5
            
            # ä½¿ç”¨å…³é”®è¯å¢å¼ºç½®ä¿¡åº¦
            enhanced_result = self._enhance_with_keywords(user_input, intent, confidence_score)
            
            return enhanced_result
            
        except Exception as e:
            logger.error(f"æ„å›¾è¯†åˆ«å¤±è´¥: {e}")
            return self._fallback_intent_recognition(user_input)
    
    def _enhance_with_keywords(self, user_input: str, intent: str, confidence: float) -> Dict[str, Any]:
        """ä½¿ç”¨å…³é”®è¯å¢å¼ºè¯†åˆ«ç»“æœ"""
        keyword_boost = self._calculate_keyword_boost(user_input, intent)
        enhanced_confidence = min(confidence + keyword_boost, 0.95)
        
        # æå–æ§½ä½ä¿¡æ¯
        slots = self._extract_slots(user_input, intent)
        
        # å¦‚æœå…³é”®è¯åŒ¹é…å¾ˆå¼ºï¼Œå¯ä»¥è¦†ç›–æ¨¡å‹ç»“æœ
        if keyword_boost > 0.3 and enhanced_confidence > 0.7:
            model_used = "enhanced_neural_network"
        elif confidence < 0.6:
            # ç½®ä¿¡åº¦å¤ªä½ï¼Œä½¿ç”¨å›é€€
            return self._fallback_intent_recognition(user_input)
        else:
            model_used = "neural_network"
        
        return {
            "intent": intent,
            "slots": slots,
            "confidence": enhanced_confidence,
            "model_used": model_used
        }
    
    def _calculate_keyword_boost(self, user_input: str, intent: str) -> float:
        """è®¡ç®—å…³é”®è¯å¢å¼ºåˆ†æ•°"""
        user_input_lower = user_input.lower()
        boost = 0.0
        
        if intent == "è”ç³»åšä¸»":
            contact_matches = sum(1 for keyword in self.contact_keywords if keyword in user_input_lower)
            boost = contact_matches * 0.1
        
        elif intent == "æŠ€æœ¯é—®ç­”":
            # æ£€æŸ¥æŠ€æœ¯å…³é”®è¯
            for keywords in self.tech_keywords.values():
                if any(keyword in user_input_lower for keyword in keywords):
                    boost += 0.1
                    break
            
            # æ£€æŸ¥é—®é¢˜ç±»å‹å…³é”®è¯
            for keywords in self.question_types.values():
                if any(keyword in user_input_lower for keyword in keywords):
                    boost += 0.1
                    break
        
        return min(boost, 0.3)  # æœ€å¤§å¢å¼º0.3
    
    def _extract_slots(self, text: str, intent: str) -> Dict[str, str]:
        """æå–æ§½ä½ä¿¡æ¯"""
        slots = {}
        text_lower = text.lower()
        
        if intent == "æŠ€æœ¯é—®ç­”":
            self._extract_technology_slots(text_lower, slots)
            self._extract_question_slots(text_lower, slots)
        
        elif intent == "åšå®¢å†…å®¹æŸ¥è¯¢":
            self._extract_content_slots(text_lower, slots)
        
        elif intent == "è”ç³»åšä¸»":
            self._extract_contact_slots(text_lower, slots)
        
        elif intent == "ä¸ªäººå’¨è¯¢":
            self._extract_personal_slots(text_lower, slots)
        
        elif intent == "ä¸€èˆ¬èŠå¤©":
            self._extract_chat_slots(text_lower, slots)
        
        return slots
    
    def _extract_technology_slots(self, text_lower: str, slots: Dict[str, str]):
        """æå–æŠ€æœ¯ç›¸å…³æ§½ä½"""
        for tech_type, keywords in self.tech_keywords.items():
            if any(keyword in text_lower for keyword in keywords):
                slots["technology_type"] = tech_type
                break
    
    def _extract_question_slots(self, text_lower: str, slots: Dict[str, str]):
        """æå–é—®é¢˜ç±»å‹æ§½ä½"""
        for q_type, keywords in self.question_types.items():
            if any(keyword in text_lower for keyword in keywords):
                slots["question_type"] = q_type
                break
    
    def _extract_content_slots(self, text_lower: str, slots: Dict[str, str]):
        """æå–å†…å®¹ç±»å‹æ§½ä½"""
        content_types = {
            "æŠ€æœ¯æ•™ç¨‹": ["æ•™ç¨‹", "æŒ‡å—", "æ•™å­¦", "æ€ä¹ˆ", "å¦‚ä½•"],
            "å­¦ä¹ ç¬”è®°": ["ç¬”è®°", "å­¦ä¹ ", "è®°å½•", "æ€»ç»“"],
            "ä¸ªäººæ—¥è®°": ["æ—¥è®°", "å¿ƒæƒ…", "æ„Ÿæ‚Ÿ", "ç”Ÿæ´»"],
            "é—®é¢˜è§£å†³": ["è§£å†³", "é—®é¢˜", "é”™è¯¯", "bug", "æ•…éšœ"]
        }
        
        for c_type, keywords in content_types.items():
            if any(keyword in text_lower for keyword in keywords):
                slots["content_type"] = c_type
                break
    
    def _extract_contact_slots(self, text_lower: str, slots: Dict[str, str]):
        """æå–è”ç³»æ–¹å¼æ§½ä½"""
        contact_methods = {
            "é‚®ç®±": ["é‚®ç®±", "email", "mail"],
            "å¾®ä¿¡": ["å¾®ä¿¡", "wechat"],
            "QQ": ["qq", "æ‰£æ‰£"],
            "äººå·¥æœåŠ¡": ["äººå·¥", "å®¢æœ", "æ”¯æŒ"]
        }
        
        for method, keywords in contact_methods.items():
            if any(keyword in text_lower for keyword in keywords):
                slots["contact_method"] = method
                break
    
    def _extract_personal_slots(self, text_lower: str, slots: Dict[str, str]):
        """æå–ä¸ªäººå’¨è¯¢æ§½ä½"""
        personal_aspects = {
            "å·¥ä½œ": ["å·¥ä½œ", "èŒä¸š", "å²—ä½", "å…¬å¸", "ä¸Šç­"],
            "å­¦ä¹ ": ["å­¦ä¹ ", "å­¦ä¹ ç»å†", "æ•™è‚²", "å­¦æ ¡", "è¯¾ç¨‹"],
            "ç”Ÿæ´»": ["ç”Ÿæ´»", "æ—¥å¸¸", "çˆ±å¥½", "å…´è¶£", "ä¹ æƒ¯"],
            "è§„åˆ’": ["è§„åˆ’", "è®¡åˆ’", "ç›®æ ‡", "æœªæ¥", "å‘å±•"],
            "å»ºè®®": ["å»ºè®®", "æ„è§", "æ¨è", "æŒ‡å¯¼", "å¸®åŠ©"]
        }
        
        for aspect, keywords in personal_aspects.items():
            if any(keyword in text_lower for keyword in keywords):
                slots["aspect"] = aspect
                break
    
    def _extract_chat_slots(self, text_lower: str, slots: Dict[str, str]):
        """æå–ä¸€èˆ¬èŠå¤©æ§½ä½"""
        chat_types = {
            "é—®å€™": ["ä½ å¥½", "åœ¨å—", "æ—©ä¸Šå¥½", "æ™šä¸Šå¥½", "hi", "hello"],
            "æ„Ÿè°¢": ["è°¢è°¢", "æ„Ÿè°¢", "å¤šè°¢", "thx", "thanks"],
            "é—²èŠ": ["æœ€è¿‘", "æ€ä¹ˆæ ·", "è¿˜å¥½å—", "å¿™å—"],
            "å…³å¿ƒ": ["æ³¨æ„", "ä¿é‡", "ç…§é¡¾å¥½", "å°å¿ƒ"]
        }
        
        for chat_type, keywords in chat_types.items():
            if any(keyword in text_lower for keyword in keywords):
                slots["chat_type"] = chat_type
                break
    
    def _fallback_intent_recognition(self, user_input: str) -> Dict[str, Any]:
        """å›é€€çš„æ„å›¾è¯†åˆ«æ–¹æ³•ï¼ˆåŸºäºå…³é”®è¯ï¼‰"""
        user_input_lower = user_input.lower()
        
        # è®¡ç®—å„æ„å›¾çš„åŒ¹é…åˆ†æ•°
        intent_scores = {}
        
        # è”ç³»åšä¸»æ„å›¾
        contact_score = sum(1 for keyword in self.contact_keywords if keyword in user_input_lower)
        intent_scores["è”ç³»åšä¸»"] = contact_score
        
        # åšå®¢å†…å®¹æŸ¥è¯¢
        blog_keywords = ['åšå®¢', 'æ–‡ç« ', 'å¸–å­', 'blog', 'post', 'å†™ä½œ', 'å†…å®¹']
        blog_score = sum(1 for keyword in blog_keywords if keyword in user_input_lower)
        intent_scores["åšå®¢å†…å®¹æŸ¥è¯¢"] = blog_score
        
        # æŠ€æœ¯é—®ç­”
        tech_score = 0
        for keywords_list in [self.tech_keywords, self.question_types]:
            for keywords in keywords_list.values():
                if any(keyword in user_input_lower for keyword in keywords):
                    tech_score += 1
        intent_scores["æŠ€æœ¯é—®ç­”"] = tech_score
        
        # ä¸ªäººå’¨è¯¢
        personal_keywords = ['ä½ ', 'åšä¸»', 'ä¸ªäºº', 'ç»å†', 'å·¥ä½œ', 'å­¦ä¹ ', 'ç”Ÿæ´»', 'å»ºè®®']
        personal_score = sum(1 for keyword in personal_keywords if keyword in user_input_lower)
        intent_scores["ä¸ªäººå’¨è¯¢"] = personal_score
        
        # ä¸€èˆ¬èŠå¤©
        chat_keywords = ['ä½ å¥½', 'åœ¨å—', 'è°¢è°¢', 'å†è§', 'æ—©ä¸Šå¥½', 'æ™šä¸Šå¥½', 'hi', 'hello']
        chat_score = sum(1 for keyword in chat_keywords if keyword in user_input_lower)
        intent_scores["ä¸€èˆ¬èŠå¤©"] = chat_score
        
        # é€‰æ‹©å¾—åˆ†æœ€é«˜çš„æ„å›¾
        best_intent = max(intent_scores, key=intent_scores.get)
        best_score = intent_scores[best_intent]
        
        # è®¡ç®—ç½®ä¿¡åº¦
        if best_score == 0:
            confidence = 0.7  # é»˜è®¤ç½®ä¿¡åº¦
            best_intent = "æŠ€æœ¯é—®ç­”"
        else:
            confidence = min(0.7 + best_score * 0.1, 0.9)
        
        # æå–æ§½ä½
        slots = self._extract_slots(user_input, best_intent)
        
        return {
            "intent": best_intent,
            "slots": slots,
            "confidence": confidence,
            "model_used": "keyword_fallback"
        }
    
    def is_contact_intent(self, user_input: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºè”ç³»åšä¸»æ„å›¾"""
        intent_result = self.recognize_intent(user_input)
        return intent_result.get("intent") == "è”ç³»åšä¸»"
    
    def get_contact_response(self, user_input: str) -> str:
        """ç”Ÿæˆè”ç³»åšä¸»çš„å“åº”"""
        intent_result = self.recognize_intent(user_input)
        
        if intent_result.get("intent") == "è”ç³»åšä¸»":
            slots = intent_result.get("slots", {})
            contact_method = slots.get("contact_method", "ä¸€èˆ¬è”ç³»")
            
            response = f"""æˆ‘è¯†åˆ«åˆ°æ‚¨æƒ³è¦{contact_method}ã€‚ä»¥ä¸‹æ˜¯è”ç³»åšä¸»çš„æ–¹å¼ï¼š

ğŸ“§ é‚®ç®±ï¼šjasonh0401@163.com
ğŸ“± QQï¼š2983105040

è¯·é€‰æ‹©é€‚åˆæ‚¨çš„æ–¹å¼è”ç³»ï¼Œåšä¸»ä¼šå°½å¿«å›å¤æ‚¨ï¼"""
            
            return response
        else:
            return "å½“å‰æœªè¯†åˆ«åˆ°è”ç³»åšä¸»çš„æ„å›¾ã€‚"


# åˆ›å»ºå…¨å±€å®ä¾‹
_intent_recognizer: Optional[SimpleIntentRecognizer] = None

def get_intent_recognizer() -> SimpleIntentRecognizer:
    """è·å–æ„å›¾è¯†åˆ«å™¨å®ä¾‹ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰"""
    global _intent_recognizer
    if _intent_recognizer is None:
        _intent_recognizer = SimpleIntentRecognizer()
    return _intent_recognizer

def recognize_intent(user_input: str) -> Dict[str, Any]:
    """è¯†åˆ«ç”¨æˆ·æ„å›¾çš„ä¾¿æ·å‡½æ•°"""
    recognizer = get_intent_recognizer()
    return recognizer.recognize_intent(user_input)

def is_contact_intent(user_input: str) -> bool:
    """åˆ¤æ–­æ˜¯å¦ä¸ºè”ç³»åšä¸»æ„å›¾çš„ä¾¿æ·å‡½æ•°"""
    recognizer = get_intent_recognizer()
    return recognizer.is_contact_intent(user_input)

def get_contact_response(user_input: str) -> str:
    """è·å–è”ç³»åšä¸»å“åº”çš„ä¾¿æ·å‡½æ•°"""
    recognizer = get_intent_recognizer()
    return recognizer.get_contact_response(user_input)

# å¯¼å‡ºä¸»è¦åŠŸèƒ½
__all__ = [
    'SimpleIntentRecognizer', 
    'recognize_intent', 
    'is_contact_intent', 
    'get_contact_response'
]
